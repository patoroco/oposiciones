{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce3a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "from datetime import datetime\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "def hash_for_obj(obj):\n",
    "    return hashlib.md5(str(obj).encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def now_in_string():\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "\n",
    "def get_type_and_tribunal(long_name):\n",
    "    \"\"\"\n",
    "    Parses the name of each tribunal, and returns the name and the speciality\n",
    "    \"\"\"\n",
    "    result = re.search('([\\s\\w.]*) - ([\\w]*)', long_name)\n",
    "    return result.groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def load_json_file(path):\n",
    "    try:\n",
    "        with open(path) as json_file:\n",
    "            return json.load(json_file)\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    \n",
    "def save_to_json_file(path, dictionary):\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(dictionary, outfile)\n",
    "\n",
    "\n",
    "def get_all_specialties():\n",
    "    CACHE_FILE = 'cache/specialities.json'\n",
    "    cache = load_json_file(CACHE_FILE)\n",
    "    if cache:\n",
    "        return cache\n",
    "    \n",
    "    DOCENTES_URL = \"https://ceice.gva.es/auto/Actas/\"\n",
    "    r = requests.get(DOCENTES_URL)\n",
    "    bs_content = BeautifulSoup(r.text, 'lxml')\n",
    "    table = bs_content.find('table', attrs={'id': 'indexlist'})\n",
    "    rows = table.findChildren('tr')\n",
    "    \n",
    "    structured_rows = []\n",
    "    for row in rows:\n",
    "        if not row.get('class')[0] in ['even', 'odd']:\n",
    "            continue\n",
    "        cols = row.findChildren('td')\n",
    "        name = cols[1].a.get_text().strip().replace('.', '').replace('/', '')\n",
    "        structured = {\n",
    "            'name': name,\n",
    "            'link': urllib.parse.urljoin(DOCENTES_URL, cols[1].a.get('href')).strip(),\n",
    "            'modified': cols[2].get_text().strip(),\n",
    "        }\n",
    "        structured_rows.append(structured)\n",
    "\n",
    "    save_to_json_file(CACHE_FILE, structured_rows)\n",
    "    return structured_rows\n",
    "\n",
    "specialties = get_all_specialties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cb891",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from slugify import slugify\n",
    "\n",
    "\n",
    "def get_all_tribunals(speciality_row):        \n",
    "    slug = slugify(speciality_row['name'])\n",
    "    CACHE_FILE = f'cache/tribunals{slug}.json'\n",
    "    \n",
    "    cache = load_json_file(CACHE_FILE)\n",
    "    if cache:\n",
    "        return cache\n",
    "    \n",
    "    print(f\"Haciendo la peticion: {speciality_row['name']}\")\n",
    "    \n",
    "    speciality_url = speciality_row['link']\n",
    "    \n",
    "    r = requests.get(speciality_url)\n",
    "    bs_content = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "    table = bs_content.find('table', attrs={'id': 'indexlist'})\n",
    "    rows = table.findChildren('tr')\n",
    "    \n",
    "    structured_rows = []\n",
    "    for row in rows:\n",
    "        if not row.get('class')[0] in ['even', 'odd']:\n",
    "            continue\n",
    "        cols = row.findChildren('td')\n",
    "        name = cols[1].a.get_text().strip().replace('.', '')\n",
    "        \n",
    "        structured = {\n",
    "            'type': get_type_and_tribunal(name)[0].strip(),\n",
    "            'tribunal': get_type_and_tribunal(name)[1],\n",
    "            'name': name,\n",
    "            'link': urllib.parse.urljoin(speciality_url, cols[1].a.get('href')).strip(),\n",
    "            'modified': cols[2].get_text().strip(),\n",
    "        }\n",
    "        structured_rows.append(structured)\n",
    "\n",
    "    save_to_json_file(CACHE_FILE, structured_rows)    \n",
    "    return structured_rows\n",
    "\n",
    "\n",
    "def get_all_files(structured_row, include_patterns=['ActaNotesProva2Provisional.pdf']):\n",
    "    slug = slugify(f\"files_tribunal_{structured_row['name']}{hash_for_obj(include_patterns)}\")\n",
    "    CACHE_FILE = f'cache/{slug}a.json'\n",
    "    \n",
    "    cache = load_json_file(CACHE_FILE)\n",
    "    if cache:\n",
    "        return cache\n",
    "    \n",
    "    print(f\"Haciendo la peticion: {CACHE_FILE}\")\n",
    "    \n",
    "    url_files = structured_row['link']\n",
    "    r = requests.get(url_files)\n",
    "    bs_content = BeautifulSoup(r.text, 'lxml')\n",
    "    table = bs_content.find('table', attrs={'id': 'indexlist'})\n",
    "    rows = table.findChildren('tr')\n",
    "    \n",
    "    structured_rows = []\n",
    "    for row in rows:\n",
    "        if not row.get('class')[0] in ['even', 'odd']:\n",
    "            continue\n",
    "        cols = row.findChildren('td')\n",
    "        name = cols[1].a.get_text().strip()\n",
    "        \n",
    "        # Only include expected files\n",
    "        included = False\n",
    "        for included_pattern in include_patterns:\n",
    "            if included_pattern in name:\n",
    "                included = True\n",
    "\n",
    "        if not included:\n",
    "            continue\n",
    "\n",
    "        structured = {\n",
    "            '_tribunal': structured_row,\n",
    "            'name': name,\n",
    "            'link': urllib.parse.urljoin(url_files, cols[1].a.get('href')),\n",
    "            'modified': cols[2].get_text().strip(),\n",
    "        }\n",
    "        structured_rows.append(structured)\n",
    "\n",
    "    save_to_json_file(CACHE_FILE, structured_rows)    \n",
    "    return structured_rows\n",
    "\n",
    "def download_file(structured_row):\n",
    "    tribunal_type = structured_row['_tribunal']['type']\n",
    "    tribunal = structured_row['_tribunal']['tribunal']\n",
    "    modified = structured_row['modified'].replace(':', '_').replace('-', '_')\n",
    "    url = structured_row['link']\n",
    "    name = Path(structured_row['name'])\n",
    "    name_wo_ext = name.with_suffix('')\n",
    "    name_ext = name.suffix\n",
    "    file_name = f\"{tribunal} - {name_wo_ext} {modified}{name_ext}\"\n",
    "    final_name = f\"{tribunal_type}/{file_name}\"\n",
    "    \n",
    "    path = Path(final_name)\n",
    "    if path.is_file():\n",
    "        # Skip the download if the file exists\n",
    "        return final_name\n",
    "    \n",
    "    r_doc = requests.get(url)\n",
    "    open(final_name, 'wb').write(r_doc.content)\n",
    "    print(f\"\\t{file_name}\")\n",
    "\n",
    "    return final_name\n",
    "\n",
    "\n",
    "start_time_str = now_in_string()\n",
    "\n",
    "for speciality in specialties:\n",
    "    tribunals = get_all_tribunals(speciality)\n",
    "    print(f\"===== START {speciality['name']}\")\n",
    "    \n",
    "    for idx, tribunal in enumerate(tribunals):\n",
    "        # Create the directory for storing the PDFs\n",
    "        tribunal_type = tribunal['type']\n",
    "        exists = os.path.exists(tribunal_type)\n",
    "        if not exists:\n",
    "            os.makedirs(tribunal_type)\n",
    "        \n",
    "        counter_str = f\"{idx}/{len(tribunals) - 1}\"\n",
    "        print(f\"{now_in_string()} ({counter_str}) {tribunal['type']} {tribunal['tribunal']}\")\n",
    "        \n",
    "        include_patterns=[\n",
    "            'ActaNotes1Provisional.pdf',\n",
    "            'ActaNotes1Definitiva.pdf',\n",
    "            'ActaNotesProva2Provisional.pdf',\n",
    "            'ActaNotesProva2Definitiva.pdf'\n",
    "        ]\n",
    "        files = get_all_files(tribunal, include_patterns=include_patterns)\n",
    "        \n",
    "        for file in files:\n",
    "            download_file(file)\n",
    "        \n",
    "    print(f\"===== END {speciality['name']}\")\n",
    "\n",
    "print(f\"===== END ({start_time_str} -> {now_in_string()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bf83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tabula\n",
    "\n",
    "CSV_PATH = \"./csv/tmp/\"\n",
    "CACHE_FILES = \"./cache/tabula/\" \n",
    "\n",
    "def extract_data_from_pdf_to_csv(file, suffix):\n",
    "    result = re.search('([\\w]*) - ([\\w]*)', file.name)\n",
    "    specialty = str(file.parent).strip()\n",
    "    tribunal = result[1]    \n",
    "    \n",
    "    CACHE_FILE = Path().joinpath(CACHE_FILES, f\"{specialty}_{tribunal}_{hash_for_obj(file.name)}.csv\")\n",
    "    path = Path(CACHE_FILE)\n",
    "    if path.is_file():\n",
    "        # Skip the download if the file exists\n",
    "        print(f\"Skipping tabula for {file}\")\n",
    "        return\n",
    "    \n",
    "    tables = tabula.read_pdf(file,pages=\"all\")\n",
    "    print(f\"({len(tables)} tables on this file)\")\n",
    "    \n",
    "    for idx, table in enumerate(tables):\n",
    "        table['specialty'] = [specialty for x in range(len(table))]\n",
    "        table['tribunal'] = [tribunal for x in range(len(table))]\n",
    "        filename = Path().joinpath(CSV_PATH, f\"{specialty}_{tribunal}_{suffix}_{idx}.csv\")\n",
    "        table.to_csv(filename, index=False)\n",
    "    \n",
    "    save_to_json_file(CACHE_FILE, {'date': now_in_string()})\n",
    "\n",
    "def extract_data_from_pattern(glob_pattern, suffix):\n",
    "    p = Path('.').glob(glob_pattern)\n",
    "    files = [x for x in p if x.is_file()]\n",
    "    for idx, file in enumerate(files):\n",
    "        counter_str = f\"{idx}/{len(files)}\"\n",
    "        print(f\"{now_in_string()} ({counter_str}) Reading '{file}'...\")\n",
    "        extract_data_from_pdf_to_csv(file, suffix)\n",
    "\n",
    "    print(\"FINISH\")\n",
    "\n",
    "\n",
    "exists = os.path.exists(CACHE_FILES)\n",
    "if not exists:\n",
    "    os.makedirs(CACHE_FILES)\n",
    "\n",
    "\n",
    "# Just to load the methods on this cell without running it\n",
    "# Create the directory for storing the CSVs\n",
    "exists = os.path.exists(CSV_PATH)\n",
    "if not exists:\n",
    "    os.makedirs(CSV_PATH)\n",
    "\n",
    "    \n",
    "include_patterns=[\n",
    "    ('**/*ActaNotes1Provisional*.pdf', 'provisional-1'),\n",
    "    ('**/*ActaNotes1Definitiva*.pdf', 'definitiva-1'),\n",
    "    ('**/*ActaNotesProva2Provisional*.pdf', 'provisional-2'),\n",
    "    ('**/*ActaNotesProva2Definitiva*.pdf', 'definitiva-2')\n",
    "]\n",
    "\n",
    "    \n",
    "start_time_str = now_in_string()\n",
    "print(f\"===== START {start_time_str}\")\n",
    "for pattern in include_patterns:\n",
    "    extract_data_from_pattern(pattern[0], suffix=pattern[1])\n",
    "print(f\"===== END ({start_time_str} -> {now_in_string()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bcc78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# This step requires running a couple of commands manually:\n",
    "# (it could be included on this jupyter, but sometimes the shell is faster :)\n",
    "#\n",
    "#\n",
    "# cat csv/tmp/*provisional-1*.csv | grep \"\\*\\*\\*\" > csv/PROVISIONAL1.csv\n",
    "# cat csv/tmp/*provisional-2*.csv | grep \"\\*\\*\\*\" > csv/PROVISIONAL2.csv\n",
    "# cat csv/tmp/*definitiva1-1*.csv | grep \"\\*\\*\\*\" > csv/DEFINITIVA1.csv\n",
    "# cat csv/tmp/*definitiva-2*.csv | grep \"\\*\\*\\*\" > csv/DEFINITIVA2.csv\n",
    "\n",
    "COLS_PRUEBA1 = ['DNI', 'nombre', 'tema', 'caso', 'total', 'especialidad', 'tribunal']\n",
    "COLS_PRUEBA2 = ['DNI', 'nombre', 'program', 'especialidad', 'tribunal']\n",
    "\n",
    "def load_csv_file(filename, cols_names):\n",
    "    return pd.read_csv(filename, names=cols_names)\n",
    "\n",
    "\n",
    "def field_is_equal(row, field_name_1, field_name_2):\n",
    "    str_1 = str(row[field_name_1])\n",
    "    str_2 = str(row[field_name_2])\n",
    "    \n",
    "    return str_1 == str_2\n",
    "\n",
    "\n",
    "def are_equal_provisional_and_definitive(field_name):\n",
    "    def return_function(row):\n",
    "        return field_is_equal(row, f'{field_name}_prov', f'{field_name}_def')\n",
    "    \n",
    "    return return_function\n",
    "\n",
    "def modifying_approval_part_1(row):\n",
    "    str_1 = str(row['total_prov'])\n",
    "    str_2 = str(row['total_def'])\n",
    "    \n",
    "    if str_1 in ['nan', 'NP'] and str_2 not in ['nan', 'NP']:\n",
    "        return \"aprobado\"\n",
    "\n",
    "    if str_2 in ['nan', 'NP'] and str_1 not in ['nan', 'NP']:\n",
    "        return \"suspenso\"\n",
    "    \n",
    "    return \"-\"\n",
    "\n",
    "def find_duplicated(total_dataset):\n",
    "    dupli_dict = {}\n",
    "    \n",
    "    duplicates = total_dataset[total_dataset.duplicated(subset=['DNI','especialidad', 'tribunal'], keep=False)]\n",
    "    for idx, dup in duplicates.iterrows():\n",
    "        # For each duplicated element, I'm going to look for the rest of occurences of that value\n",
    "        #Â and compare the name. If the name is partially similar, I'll include in the results for debug them\n",
    "        name = dup['nombre']\n",
    "\n",
    "        others = duplicates[(\n",
    "            (duplicates['DNI'] == dup['DNI'])\n",
    "            & (duplicates['especialidad'] == dup['especialidad'])\n",
    "            & (duplicates['tribunal'] == dup['tribunal'])\n",
    "            & (duplicates['nombre'] != dup['nombre'])\n",
    "        )]\n",
    "\n",
    "        for ido, other in others.iterrows():\n",
    "            name_other = other['nombre']\n",
    "\n",
    "            if name in name_other:\n",
    "                dupli_dict[name] = name_other\n",
    "    return dupli_dict\n",
    "\n",
    "################\n",
    "\n",
    "start_time_str = now_in_string()\n",
    "print(f\"===== START {start_time_str}\")\n",
    "\n",
    "provisional_1 = load_csv_file('csv/PROVISIONAL1.csv', COLS_PRUEBA1)\n",
    "definitiva_1 = load_csv_file('csv/DEFINITIVA1.csv', COLS_PRUEBA1)\n",
    "\n",
    "provisional_2 = load_csv_file('csv/PROVISIONAL2.csv', COLS_PRUEBA2)\n",
    "definitiva_2 = load_csv_file('csv/DEFINITIVA2.csv', COLS_PRUEBA2)\n",
    "\n",
    "\n",
    "prueba_1 = pd.merge(provisional_1, definitiva_1, on=['DNI', 'especialidad', 'tribunal', 'nombre'], how=\"outer\", suffixes=('_prov', '_def'))\n",
    "prueba_2 = pd.merge(provisional_2, definitiva_2, on=['DNI', 'especialidad', 'tribunal', 'nombre'], how=\"outer\", suffixes=('_prov', '_def'))\n",
    "\n",
    "total = pd.merge(prueba_1, prueba_2, on=['DNI', 'especialidad', 'tribunal', 'nombre'], how=\"outer\", suffixes=('_1', '_2'))              \n",
    "\n",
    "\n",
    "duplicados = find_duplicated(total)\n",
    "print(f\"Rarezas encontradas: {len(duplicados)}\")\n",
    "\n",
    "# Fix the names with 'rarezas'\n",
    "for short_name, long_name in duplicados.items():\n",
    "    prueba_1.replace(short_name, long_name, inplace=True)\n",
    "    prueba_2.replace(short_name, long_name, inplace=True)\n",
    "\n",
    "total_fixed = pd.merge(prueba_1, prueba_2, on=['DNI', 'especialidad', 'tribunal', 'nombre'], how=\"outer\", suffixes=('_1', '_2'))\n",
    "duplicados_after_fixing = find_duplicated(total_fixed)\n",
    "print(f\"Rarezas pendientes: {len(duplicados_after_fixing)}\")\n",
    "\n",
    "\n",
    "print(f\"prueba 1: {len(prueba_1)}, prueba 2: {len(prueba_2)}, mergeado: {len(total_fixed)}\")\n",
    "\n",
    "\n",
    "# Add columns 'iguales' that compares provisional and definitive results\n",
    "total_fixed['tema_='] = total_fixed.apply(are_equal_provisional_and_definitive('tema'), axis=1)\n",
    "total_fixed['caso_='] = total_fixed.apply(are_equal_provisional_and_definitive('caso'), axis=1)\n",
    "total_fixed['total_p1_='] = total_fixed.apply(modifying_approval_part_1, axis=1)\n",
    "total_fixed['program_='] = total_fixed.apply(are_equal_provisional_and_definitive('program'), axis=1)\n",
    "\n",
    "# Sort the columns to look better on the spreedsheet\n",
    "total_fixed = total_fixed.reindex(columns=['especialidad', 'tribunal', 'DNI', 'nombre',\n",
    "                                           'tema_prov', 'tema_def', 'tema_=',\n",
    "                                           'caso_prov', 'caso_def', 'caso_=',\n",
    "                                           'total_prov', 'total_def', 'total_p1_=',\n",
    "                                           'program_prov', 'program_def', 'program_='\n",
    "                                          ])\n",
    "\n",
    "fila = total_fixed[total_fixed['total_p1_='] == 'suspenso']\n",
    "fila\n",
    "\n",
    "print(f\"===== END ({start_time_str} -> {now_in_string()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1273b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_str = now_in_string()\n",
    "print(f\"===== START {start_time_str}\")\n",
    "\n",
    "# Delete the name column (for sharing):\n",
    "del total_fixed['nombre']\n",
    "\n",
    "with pd.ExcelWriter('results_sin_nombre.xlsx') as writter:\n",
    "    for speciality in total_fixed['especialidad'].unique():\n",
    "        temp_df = total_fixed.loc[total_fixed['especialidad'] == speciality]\n",
    "        del temp_df['especialidad']\n",
    "        temp_df.to_excel(writter, sheet_name=speciality, index=False)\n",
    "\n",
    "\n",
    "print(f\"===== END ({start_time_str} -> {now_in_string()})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (oposiciones)",
   "language": "python",
   "name": "oposiciones"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
